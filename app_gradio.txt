import gradio as gr
import sounddevice as sd
import whisper
import os
from gtts import gTTS
import tempfile
import pyaudio
import wave
from pydub import AudioSegment
from scipy.io import wavfile
from openai import OpenAI
import numpy as np
import time
import shutil
import atexit
import warnings
import json

warnings.filterwarnings('ignore', message='.*sample_rate will be ignored.*')

# Keep the same system prompt and personal context
SYSTEM_PROMPT = """# Yashaswa Varshney (YASH)
[... existing system prompt ...]"""

PERSONAL_CONTEXT = {
    "name": "Yashaswa Varshney you can call me Yash",
    "current_role": "Software Development Engineer at HyperBots",
    "education": "B.Tech in Computer Science from KIIT University 2024 Graduate",
    "skills": ["AI Agents", "Financial Systems", "Automation", "Python", "LangGraph", "RAG"],
    "projects": [
        "Multi-Database Query Orchestrator",
        "Sector Rotation Graph",
        "FilthyFilter"
    ],
    "interests": ["Blogging (Finance, AI)", "Himalayan trekking", "Swimming", "Hiking"],
    "achievements": [
        "Contributed to Flowise Python connectors",
        "Blog on Nadaraya-Watson Indicator with 12.5K+ views",
        "Speaker at UN Workshop on Power BI",
        "ArtStation Utopia Concept Art Winner"
    ]
}

# Initialize OpenAI client and Whisper model
try:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable not found. Please set it in your .env file.")
    client = OpenAI(api_key=api_key)
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    raise

model = whisper.load_model("base")

AUDIO_DIR = "audio_files"
os.makedirs(AUDIO_DIR, exist_ok=True)

# Global state for conversation history
conversation_history = []
MEMORY_WINDOW_SIZE = 5

def cleanup_audio_files():
    """Clean up all audio files"""
    for file in os.listdir(AUDIO_DIR):
        try:
            os.remove(os.path.join(AUDIO_DIR, file))
        except Exception as e:
            print(f"Error cleaning up {file}: {e}")

atexit.register(cleanup_audio_files)

def record_audio(duration=6):
    """Record audio using sounddevice"""
    audio = sd.rec(int(duration * 16000), samplerate=16000, channels=1)
    sd.wait()
    return audio

def transcribe(audio):
    """Transcribe audio using Whisper"""
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        filename = f.name
        wavfile.write(filename, 16000, audio)
    result = model.transcribe(filename)
    os.unlink(filename)
    return result["text"]

def get_conversation_context():
    """Get the last N turns of conversation for context"""
    if not conversation_history:
        return []
    
    recent_turns = conversation_history[-MEMORY_WINDOW_SIZE:]
    return recent_turns

def chat_with_gpt(user_input):
    """Chat with GPT using conversation context"""
    conversation_context = get_conversation_context()
    
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "system", "content": f"Personal Context: {PERSONAL_CONTEXT}"}
    ]
    
    messages.extend(conversation_context)
    messages.append({"role": "user", "content": user_input})
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=messages,
        temperature=0.7
    )
    return response.choices[0].message.content

def speak(text):
    """Convert text to speech and return audio file path"""
    try:
        tts = gTTS(text=text, lang='en', slow=False)
        mp3_path = os.path.join(AUDIO_DIR, f"temp_{int(time.time())}_{np.random.randint(1000)}.mp3")
        wav_path = os.path.join(AUDIO_DIR, f"temp_{int(time.time())}_{np.random.randint(1000)}.wav")
        
        tts.save(mp3_path)
        sound = AudioSegment.from_mp3(mp3_path)
        sound = sound.speedup(playback_speed=1.2)
        sound.export(wav_path, format="wav")
        
        os.unlink(mp3_path)
        return wav_path
    except Exception as e:
        print(f"Error in speech synthesis: {e}")
        return None

def process_text_input(text, history):
    """Process text input and generate response"""
    if not text.strip():
        return history, ""
    
    # Add user message to history
    history.append((text, ""))
    
    # Get AI response
    response = chat_with_gpt(text)
    
    # Generate audio for response
    audio_path = speak(response)
    
    # Update history with AI response
    history[-1] = (text, response)
    
    # Add to conversation history
    conversation_history.append({"role": "user", "content": text})
    conversation_history.append({"role": "assistant", "content": response})
    
    return history, "", audio_path if audio_path else None

def process_voice_input(audio):
    """Process voice input and generate response"""
    if audio is None:
        return None, None, None
    
    # Convert audio to numpy array
    audio_data = audio[1]
    
    # Transcribe audio
    text = transcribe(audio_data)
    
    if not text.strip():
        return None, "No speech detected. Please try again.", None
    
    # Process the transcribed text
    history = []
    history, _, audio_path = process_text_input(text, history)
    
    return text, history[0][1] if history else "", audio_path

def clear_chat():
    """Clear the chat history"""
    global conversation_history
    conversation_history = []
    return None, None, None

# Create Gradio interface
with gr.Blocks(title="Chat with Yash", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# Chat with Yash")
    
    with gr.Row():
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(
                label="Conversation",
                height=600,
                show_copy_button=True
            )
            
            with gr.Row():
                with gr.Column(scale=4):
                    text_input = gr.Textbox(
                        label="Type your message",
                        placeholder="Type your question here...",
                        lines=3
                    )
                with gr.Column(scale=1):
                    text_submit = gr.Button("Send", variant="primary")
            
            with gr.Row():
                voice_input = gr.Audio(
                    label="Or record your voice",
                    type="numpy",
                    sources=["microphone"],
                    duration=6
                )
                voice_submit = gr.Button("Send Voice", variant="primary")
            
            clear_button = gr.Button("Clear Chat", variant="secondary")
            
            audio_output = gr.Audio(label="AI Response", type="filepath", visible=True)
        
        with gr.Column(scale=1):
            gr.Markdown("""
            ### Connect with Yash
            
            üìß **Email**: [yswa.var@icloud.com](mailto:yswa.var@icloud.com)  
            üì± **Phone**: [+91 6396300355](tel:+916396300355)  
            üíº **LinkedIn**: [linkedin.com/in/yashaswa-varshney](https://linkedin.com/in/yashaswa-varshney)  
            üë®‚Äçüíª **GitHub**: [github.com/yswa-var](https://github.com/yswa-var)  
            """)
    
    # Set up event handlers
    text_submit.click(
        process_text_input,
        inputs=[text_input, chatbot],
        outputs=[chatbot, text_input, audio_output]
    )
    
    text_input.submit(
        process_text_input,
        inputs=[text_input, chatbot],
        outputs=[chatbot, text_input, audio_output]
    )
    
    voice_submit.click(
        process_voice_input,
        inputs=[voice_input],
        outputs=[text_input, chatbot, audio_output]
    )
    
    clear_button.click(
        clear_chat,
        outputs=[chatbot, text_input, audio_output]
    )

if __name__ == "__main__":
    demo.launch(share=True) 